{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d5411c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import date\n",
    "from pdfminer.high_level import extract_text\n",
    "from google.colab import drive\n",
    "\n",
    "# ================ MOUNT GOOGLE DRIVE ==================\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# ================== SETUP =============================\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0\"\n",
    "}\n",
    "\n",
    "# ================= PATH ==============================\n",
    "def create_path(folder_name):\n",
    "    path = os.path.join(\"/content/drive/MyDrive/Penalaran Komputer\", folder_name)\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    return path\n",
    "\n",
    "# ================= SCRAPING ===========================\n",
    "def open_page(link):\n",
    "    for _ in range(5):\n",
    "        try:\n",
    "            r = requests.get(link, headers=HEADERS, timeout=30)\n",
    "            r.raise_for_status()\n",
    "            time.sleep(2)\n",
    "            return BeautifulSoup(r.text, \"lxml\")\n",
    "        except:\n",
    "            time.sleep(3)\n",
    "    return None\n",
    "\n",
    "def normalize_link(link):\n",
    "    return link if link.startswith(\"http\") else \"https://putusan3.mahkamahagung.go.id\" + link\n",
    "\n",
    "def get_detail(soup, keyword):\n",
    "    td = soup.find(lambda tag: tag.name == \"td\" and keyword.lower() in tag.text.lower())\n",
    "    return td.find_next().get_text(strip=True) if td else \"\"\n",
    "\n",
    "def get_pdf(url, path_pdf):\n",
    "    try:\n",
    "        r = requests.get(url, headers=HEADERS)\n",
    "        r.raise_for_status()\n",
    "        fname = os.path.basename(url)\n",
    "        fpath = os.path.join(path_pdf, fname)\n",
    "        with open(fpath, \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "        print(f\"Downloaded: {fname}\")\n",
    "        return fpath, fname\n",
    "    except:\n",
    "        return None, None\n",
    "\n",
    "def extract_data(link, path_output, path_pdf):\n",
    "    soup = open_page(normalize_link(link))\n",
    "    if not soup: return None, False\n",
    "\n",
    "    table = soup.find(\"table\", {\"class\": \"table\"})\n",
    "    if not table: return None, False\n",
    "\n",
    "    judul = table.find(\"h2\").text.strip() if table.find(\"h2\") else \"\"\n",
    "\n",
    "    fields = [\n",
    "        \"Nomor\", \"Tingkat Proses\", \"Klasifikasi\", \"Kata Kunci\", \"Tahun\", \"Tanggal Register\",\n",
    "        \"Lembaga Peradilan\", \"Jenis Lembaga Peradilan\", \"Hakim Ketua\", \"Hakim Anggota\",\n",
    "        \"Panitera\", \"Amar\", \"Amar Lainnya\", \"Catatan Amar\", \"Tanggal Musyawarah\",\n",
    "        \"Tanggal Dibacakan\", \"Kaidah\", \"Status\", \"Abstrak\"\n",
    "    ]\n",
    "    values = [get_detail(table, f) for f in fields]\n",
    "\n",
    "    link_pdf_tag = soup.find(\"a\", href=re.compile(r\"/pdf/\"))\n",
    "    if not link_pdf_tag: return None, False\n",
    "    pdf_url = normalize_link(link_pdf_tag[\"href\"])\n",
    "    pdf_path, pdf_name = get_pdf(pdf_url, path_pdf)\n",
    "    if not pdf_path: return None, False\n",
    "\n",
    "    text_pdf = extract_text(pdf_path)\n",
    "    data = [judul] + values + [normalize_link(link), pdf_url, pdf_name, text_pdf]\n",
    "    columns = [\n",
    "        \"judul\", \"nomor\", \"tingkat_proses\", \"klasifikasi\", \"kata_kunci\", \"tahun\",\n",
    "        \"tanggal_register\", \"lembaga_peradilan\", \"jenis_lembaga_peradilan\", \"hakim_ketua\",\n",
    "        \"hakim_anggota\", \"panitera\", \"amar\", \"amar_lainnya\", \"catatan_amar\",\n",
    "        \"tanggal_musyawarah\", \"tanggal_dibacakan\", \"kaidah\", \"status\", \"abstrak\",\n",
    "        \"link\", \"link_pdf\", \"file_name_pdf\", \"text_pdf\"\n",
    "    ]\n",
    "    return pd.DataFrame([data], columns=columns), True\n",
    "\n",
    "def run_scraper():\n",
    "    path_out = create_path(\"CSV4\")\n",
    "    path_pdf = create_path(\"PDF4\")\n",
    "    today = date.today().strftime(\"%Y-%m-%d\")\n",
    "    file_csv = os.path.join(path_out, f\"putusan_fidusia_{today}.csv\")\n",
    "\n",
    "    count = 0\n",
    "    seen = set()\n",
    "    page = 1\n",
    "\n",
    "    while count < 50:\n",
    "        url = f\"https://putusan3.mahkamahagung.go.id/direktori/index/kategori/fidusia-1/page/{page}.html\" if page > 1 else \"https://putusan3.mahkamahagung.go.id/direktori/index/kategori/fidusia-1.html\"\n",
    "        soup = open_page(url)\n",
    "        if not soup: break\n",
    "\n",
    "        links = soup.find_all(\"a\", href=re.compile(\"/direktori/putusan\"))\n",
    "        for tag in links:\n",
    "            href = tag.get(\"href\")\n",
    "            if href and href not in seen:\n",
    "                seen.add(href)\n",
    "                df, ok = extract_data(href, path_out, path_pdf)\n",
    "                if ok:\n",
    "                    df.to_csv(file_csv, mode='a', header=not os.path.exists(file_csv), index=False)\n",
    "                    count += 1\n",
    "                    if count >= 50: break\n",
    "        page += 1\n",
    "    print(f\"Done. Total: {count} putusan.\")\n",
    "\n",
    "# Jalankan Scraper\n",
    "run_scraper()\n",
    "\n",
    "# ===================================\n",
    "# CLEANING CSV (Tahap 2)\n",
    "# ===================================\n",
    "\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/Penalaran Komputer/CSV4/putusan_fidusia_2025-06-24.csv\")\n",
    "\n",
    "# Bersihkan kolom text_pdf\n",
    "\n",
    "def clean_text_ma(text):\n",
    "    if pd.isna(text): return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'direktori putusan.*?transparansi.*?peradilan\\.', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'email\\s*:\\s*\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'telp.*?(\\d{3,})', '', text)\n",
    "    text = re.sub(r'nip\\.?\\s*\\d+', '', text)\n",
    "    text = re.sub(r'panitera.*?hakim.*?', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'ttd.*?', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'halaman\\s*\\d+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "df['text_pdf_cleaned'] = df['text_pdf'].apply(clean_text_ma)\n",
    "\n",
    "# Simpan hasilnya\n",
    "cleaned_path = \"/content/drive/MyDrive/Penalaran Komputer/CSV4/putusan_fidusia_cleaned_FINAL.csv\"\n",
    "df.to_csv(cleaned_path, index=False)\n",
    "print(\"✅ Cleaning selesai. File disimpan ke:\", cleaned_path)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load data dari hasil scraping\n",
    "file_path = '/content/drive/MyDrive/Penalaran Komputer/CSV4/putusan_fidusia_cleaned_FINAL.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Kolom target\n",
    "text_col = 'text_pdf_cleaned'\n",
    "\n",
    "# Pola regex lanjutan untuk membersihkan watermark, disclaimer, dan duplikasi\n",
    "def clean_advanced(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Hapus watermark & identitas MA\n",
    "    text = re.sub(r'(mahkamah agung republik indonesia|putusan\\.mahkamahagung\\.go\\.id)', ' ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'direktori putusan.*?transparansi dan akuntabilitas.*?(?=halaman|\\s+)', ' ', text, flags=re.DOTALL|re.IGNORECASE)\n",
    "\n",
    "    # Hapus penutup hakim/panitera\n",
    "    text = re.sub(r'ttd\\./.*?(?=panitera|hakim|untuk salinan|dr\\.|nip)', ' ', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'nip\\s*\\d{5,}', ' ', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Hapus 'halaman x dari x halaman' dan yang sejenis\n",
    "    text = re.sub(r'halaman\\s*\\d+\\s*(dari\\s*\\d+\\s*halaman)?', ' ', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Hapus duplikasi berulang (cut-off setelah putusan utama)\n",
    "    text = re.sub(r'putusan tersebut diucapkan.*', '', text, flags=re.DOTALL|re.IGNORECASE)\n",
    "\n",
    "    # Normalisasi spasi dan huruf besar kecil\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip().lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "def extract_amar_putusan(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Lowercase + spasi normal\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    # Mulai dari kalimat penting\n",
    "    start_idx = text.find(\"m e n g a d i l i\")\n",
    "    if start_idx == -1:\n",
    "        start_idx = text.find(\"demi keadilan berdasarkan ketuhanan yang maha esa\")\n",
    "    if start_idx == -1:\n",
    "        return text  # fallback\n",
    "\n",
    "    text = text[start_idx:]\n",
    "\n",
    "    # Akhiri di bagian-bagian administratif\n",
    "    end_phrases = [\n",
    "        \"putusan ini diucapkan\", \"diputuskan dalam rapat\",\n",
    "        \"panitera pengganti\", \"untuk salinan\", \"nip.\"\n",
    "    ]\n",
    "    for phrase in end_phrases:\n",
    "        end_idx = text.find(phrase)\n",
    "        if end_idx != -1:\n",
    "            text = text[:end_idx]\n",
    "            break\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "def finalize_cleaning(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return \"\"\n",
    "\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Cari frasa kunci amar putusan\n",
    "    start_idx = text.find('m e n g a d i l i')\n",
    "    if start_idx != -1:\n",
    "        text = text[start_idx:]  # Ambil dari \"m e n g a d i l i\"\n",
    "\n",
    "    # Hapus watermark, disclaimer, penutup jika ada\n",
    "    text = re.sub(r'direktori putusan.*?(?=menolak|memperbaiki|pada hari|putusan nomor|demi keadilan)', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'email\\s*:\\s*\\S+@\\S+|telp\\s*:\\s*[\\d\\s\\-\\(\\)]+', '', text)\n",
    "    text = re.sub(r'nip\\.\\s*\\d+|\\bpada hari.*', '', text)\n",
    "    text = re.sub(r'panitera.*?$', '', text, flags=re.DOTALL)\n",
    "\n",
    "    # Bersihkan spasi berlebih\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def cari_amar_alternatif(text):\n",
    "    text = text.lower()\n",
    "    keywords = ['m e n g a d i l i', 'putusan pengadilan', 'menjatuhkan pidana', 'memperbaiki putusan']\n",
    "    for key in keywords:\n",
    "        idx = text.find(key)\n",
    "        if idx != -1:\n",
    "            return text[idx:]\n",
    "    return text  # fallback: kembalikan teks penuh\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Terapkan ke kolom text_pdf_cleaned\n",
    "df['text_pdf_cleaned'] = df['text_pdf_cleaned'].apply(clean_advanced)\n",
    "df['text_pdf_cleaned'] = df['text_pdf_cleaned'].apply(extract_amar_putusan)\n",
    "df['text_pdf_cleaned'] = df['text_pdf_cleaned'].apply(finalize_cleaning)\n",
    "df['text_pdf_cleaned'] = df['text_pdf_cleaned'].apply(cari_amar_alternatif)\n",
    "\n",
    "\n",
    "\n",
    "# Simpan hasil baru\n",
    "output_path = '/content/drive/MyDrive/Penalaran Komputer/CSV4/putusan_fidusia_cleaned_FINAL_BERSIH_FIX.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"✅ Sukses! File dibersihkan dan disimpan ke:\")\n",
    "print(output_path)\n",
    "\n",
    "\n",
    "df[['nomor', 'text_pdf_cleaned']].head(5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
